{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVh3fjdGQN-c",
        "outputId": "f2fb4b88-662d-4473-893e-ccaa35fabd64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reviews: 1400\n",
            "Example review:\n",
            " filled with a tantalizing air of suspense , with a friend like harry is an unusual yet well-balanced mix of dark comedy , french thriller , and surreal drama . as i was watching the film , i found myself groping for its message at each turn of the plot . its ultimate effect is comparable to claude c\n",
            "Train size: 1120, Test size: 280\n",
            "Feature matrix shape: (1120, 32238)\n",
            "\n",
            "Results:\n",
            "            Classifier  Accuracy (%)\n",
            "0          Naive Bayes     80.000000\n",
            "1  Logistic Regression     83.214286\n",
            "2         SVM (Linear)     83.214286\n"
          ]
        }
      ],
      "source": [
        "import glob #to find files with matching pattern\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer #CountVectorizer is used to convert raw text into numeric feature matrix of word counts\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "#\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_path = \"mix20_rand700_tokens.zip\"\n",
        "\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref: #extracting the given data - subfolders are present- pos,neg\n",
        "    zip_ref.extractall(\"movie_reviews\")\n",
        "\n",
        "\n",
        "os.listdir(\"movie_reviews\")\n",
        "\n",
        "\n",
        "\n",
        "pos_files = glob.glob(\"movie_reviews/tokens/pos/*.txt\")\n",
        "neg_files = glob.glob(\"movie_reviews/tokens/neg/*.txt\")\n",
        "\n",
        "#latin-1 is used because there may be special characters present in the reviews\n",
        "pos_reviews = [open(f, encoding=\"latin-1\").read() for f in pos_files] #list of positive review strings\n",
        "neg_reviews = [open(f, encoding=\"latin-1\").read() for f in neg_files] #list of negative review strings\n",
        "\n",
        "texts = pos_reviews + neg_reviews\n",
        "labels = [1]*len(pos_reviews) + [0]*len(neg_reviews) #creates a list of 1's for positive reviews and list of 0's for negative reviews\n",
        "\n",
        "print(\"Total reviews:\", len(texts))\n",
        "print(\"Example review:\\n\", texts[0][:300])\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( #randome_state is used to prevent different random splits each time\n",
        "    texts, labels, test_size=0.2, random_state=42, stratify=labels #stratify is used to ensure both sets have same proportion of pos and neg\n",
        ") #test_size implies 20% of data goes to testing, rest 80% goes to training\n",
        "\n",
        "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "\n",
        "# here we extract the features, each word becomes a feature\n",
        "vectorizer = CountVectorizer(binary=True, stop_words=\"english\")  #if a word is present in revview it stores 1, else 0, stopwords is used to remove common connecting english words in general\n",
        "X_train_vec = vectorizer.fit_transform(X_train) # learning the vocabulary and turn the training reviews into vectors\n",
        "X_test_vec = vectorizer.transform(X_test) #use same vocabulary from training and turn it into vectors\n",
        "# no fit_transform is used for test because we don't want to introduce new words\n",
        "\n",
        "print(\"Feature matrix shape:\", X_train_vec.shape) #coverted to sparse vector that consists of only 0's and 1's and shows (number of samples, number of features)\n",
        "# number of samples = number of training reviews\n",
        "# number of features = number of unique words in vocabulary\n",
        "results = {}\n",
        "\n",
        "# Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train) #conditional probability happens..generative model\n",
        "y_pred_nb = nb.predict(X_test_vec)\n",
        "results[\"Naive Bayes\"] = accuracy_score(y_test, y_pred_nb) * 100\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_vec, y_train) #it depends more on learning the weights for each word, chooses the model with maximum entropy from data..discriminative\n",
        "y_pred_lr = lr.predict(X_test_vec)\n",
        "results[\"Logistic Regression\"] = accuracy_score(y_test, y_pred_lr) * 100\n",
        "\n",
        "# Support Vector Machine , finds the best separating hyperplane between positive and negative reviews  , best for classification problems\n",
        "svm = LinearSVC(max_iter=5000)\n",
        "svm.fit(X_train_vec, y_train)\n",
        "y_pred_svm = svm.predict(X_test_vec)\n",
        "results[\"SVM (Linear)\"] = accuracy_score(y_test, y_pred_svm) * 100\n",
        "\n",
        "\n",
        "df_results = pd.DataFrame(list(results.items()), columns=[\"Classifier\", \"Accuracy (%)\"])\n",
        "print(\"\\nResults:\")\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ExNSxMpSe6D"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Frequency (counts)\n",
        "vec_freq = CountVectorizer(stop_words=\"english\") #counts word occurrences and remove the common english words\n",
        "X_train_f = vec_freq.fit_transform(X_train) #learns and builds sparse matrix\n",
        "X_test_f  = vec_freq.transform(X_test)\n",
        "\n",
        "# Presence (binary)\n",
        "vec_bin = CountVectorizer(binary=True, stop_words=\"english\") #ignores the number of times the word occurs and marks only the presence of the word\n",
        "X_train_b = vec_bin.fit_transform(X_train)\n",
        "X_test_b  = vec_bin.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LolcgMvESiga"
      },
      "outputs": [],
      "source": [
        "vec_bi = CountVectorizer(binary=True, stop_words=\"english\", ngram_range=(1,2)) #for both unigrams (single word review) and bigrams(double word review)\n",
        "X_train_bi = vec_bi.fit_transform(X_train)\n",
        "X_test_bi  = vec_bi.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVGZ3iJVSsfz"
      },
      "outputs": [],
      "source": [
        "vec_bigrams = CountVectorizer(binary=True, stop_words=\"english\", ngram_range=(2,2)) #only bigrams are reviewed\n",
        "X_train_bigrams = vec_bigrams.fit_transform(X_train)\n",
        "X_test_bigrams  = vec_bigrams.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuWYSLSCSvjq"
      },
      "outputs": [],
      "source": [
        "import spacy #imports english language model and gives parts of speech tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def keep_adjectives(texts):\n",
        "    new_texts = []\n",
        "    for doc in nlp.pipe(texts, batch_size=20, disable=[\"ner\",\"parser\"]):\n",
        "        adj_words = [token.text for token in doc if token.pos_==\"ADJ\"]\n",
        "        new_texts.append(\" \".join(adj_words))\n",
        "    return new_texts\n",
        "\n",
        "X_train_adj = CountVectorizer(binary=True).fit_transform(keep_adjectives(X_train))\n",
        "X_test_adj  = CountVectorizer(binary=True).fit(keep_adjectives(X_train)).transform(keep_adjectives(X_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ---- Preprocessor for Unigrams + POS ----\n",
        "def pos_preprocessor(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([\n",
        "        f\"{token.text.lower()}_{token.pos_}\"\n",
        "        for token in doc\n",
        "        if token.is_alpha and not token.is_stop\n",
        "    ])\n",
        "\n",
        "# ---- Preprocessor for Unigrams + Position ----\n",
        "def position_preprocessor(text):\n",
        "    tokens = [w.lower() for w in text.split() if w.isalpha()]\n",
        "    return \" \".join([f\"{word}_{i}\" for i, word in enumerate(tokens)])\n"
      ],
      "metadata": {
        "id": "uQkE5k2H1Rtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhhkUpSkSyS7"
      },
      "outputs": [],
      "source": [
        "vec_top = CountVectorizer(binary=True, stop_words=\"english\", max_features=2633)\n",
        "X_train_top = vec_top.fit_transform(X_train)\n",
        "X_test_top  = vec_top.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vniwOWxfRi1",
        "outputId": "74699bcd-c835-4b02-e6b9-03f71ae2c907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Features  #Features         NB         ME        SVM\n",
            "1              Unigrams (freq)      32238  76.785714  78.214286  77.142857\n",
            "2          Unigrams (presence)      32238  80.000000  83.214286  83.214286\n",
            "3  Unigrams+Bigrams (presence)     325558  78.571429  82.857143  82.857143\n",
            "4                 Bigrams only     293320  76.428571  73.928571  74.642857\n",
            "5              Adjectives only      32238  80.000000  83.214286  83.214286\n",
            "6            Top 2633 unigrams       2633  77.142857  81.428571  81.071429\n",
            "7               Unigrams + POS      40048  81.071429  85.714286  85.714286\n",
            "8          Unigrams + Position     410241  59.285714  61.071429  60.714286\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def evaluate_models(X_train, X_test, y_train, y_test):\n",
        "    results = {}\n",
        "    nb = MultinomialNB().fit(X_train, y_train)\n",
        "    results[\"NB\"] = accuracy_score(y_test, nb.predict(X_test)) * 100\n",
        "\n",
        "    lr = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
        "    results[\"ME\"] = accuracy_score(y_test, lr.predict(X_test)) * 100\n",
        "\n",
        "    svm = LinearSVC(max_iter=5000).fit(X_train, y_train)\n",
        "    results[\"SVM\"] = accuracy_score(y_test, svm.predict(X_test)) * 100\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "experiments = [\n",
        "    (\"Unigrams (freq)\", CountVectorizer(stop_words=\"english\", binary=False)),\n",
        "    (\"Unigrams (presence)\", CountVectorizer(stop_words=\"english\", binary=True)),\n",
        "    (\"Unigrams+Bigrams (presence)\", CountVectorizer(stop_words=\"english\", binary=True, ngram_range=(1,2))),\n",
        "    (\"Bigrams only\", CountVectorizer(stop_words=\"english\", binary=True, ngram_range=(2,2))),\n",
        "    (\"Adjectives only\", CountVectorizer(stop_words=\"english\", binary=True)),\n",
        "\n",
        "    (\"Top 2633 unigrams\", CountVectorizer(stop_words=\"english\", binary=True, max_features=2633)),\n",
        "    # ✅ New experiments added below\n",
        "    (\"Unigrams + POS\", CountVectorizer(binary=True, preprocessor=pos_preprocessor)),\n",
        "    (\"Unigrams + Position\", CountVectorizer(binary=True, preprocessor=position_preprocessor)),\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, vectorizer in experiments:\n",
        "\n",
        "    Xtr = vectorizer.fit_transform(X_train)\n",
        "    Xte = vectorizer.transform(X_test)\n",
        "\n",
        "    res = evaluate_models(Xtr, Xte, y_train, y_test)\n",
        "    vocab_size = len(vectorizer.get_feature_names_out())  # number of features\n",
        "\n",
        "    results.append([name, vocab_size, res[\"NB\"], res[\"ME\"], res[\"SVM\"]])\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Features\", \"#Features\", \"NB\", \"ME\", \"SVM\"])\n",
        "df.index += 1\n",
        "print(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#LOGISTIC REGREssion\n",
        "\n",
        "# Convert sparse matrices to dense\n",
        "X_train_manual = X_train_vec.toarray()\n",
        "X_test_manual = X_test_vec.toarray()\n",
        "\n",
        "y_train_arr = np.array(y_train)\n",
        "y_test_arr = np.array(y_test)\n",
        "\n",
        "# Add bias term\n",
        "X_train_manual = np.hstack([np.ones((X_train_manual.shape[0], 1)), X_train_manual])\n",
        "X_test_manual = np.hstack([np.ones((X_test_manual.shape[0], 1)), X_test_manual])\n",
        "\n",
        "#computing sigmoid\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# cross entropy lss\n",
        "def compute_loss(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)\n",
        "    epsilon = 1e-15  # avoid log(0)\n",
        "    return - (1/m) * np.sum(y * np.log(h + epsilon) + (1 - y) * np.log(1 - h + epsilon))\n",
        "\n",
        "#Gradient Descent\n",
        "def gradient_descent(X, y, learning_rate=0.1, num_iters=1000):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(X @ theta)\n",
        "        gradient = (1/m) * (X.T @ (h - y))\n",
        "        theta -= learning_rate * gradient\n",
        "\n",
        "        current_loss = compute_loss(X, y, theta)\n",
        "        losses.append(current_loss)\n",
        "\n",
        "        if i % 100 == 0 or i == num_iters - 1:\n",
        "            print(f\"Iteration {i}: Loss = {current_loss:.6f}\")\n",
        "\n",
        "    return theta, losses\n",
        "\n",
        "#model training\n",
        "theta, losses = gradient_descent(X_train_manual, y_train_arr,\n",
        "                                 learning_rate=0.1, num_iters=1000)\n",
        "\n",
        "#Minimum Loss Achieved\n",
        "min_loss = min(losses)\n",
        "min_loss_iter = losses.index(min_loss)\n",
        "print(f\"\\nMinimum Loss: {min_loss:.6f} at Iteration {min_loss_iter}\")\n",
        "\n",
        "#Final Training Loss\n",
        "final_loss = compute_loss(X_train_manual, y_train_arr, theta)\n",
        "print(f\"Final Training Loss: {final_loss:.6f}\")\n",
        "\n",
        "#Final Optimized Weights\n",
        "print(\"\\nFirst 10 Optimized Weights (theta):\")\n",
        "print(theta[:10])  # showing only first 10 for readability\n",
        "\n",
        "#Evaluation on test data\n",
        "y_pred_prob = sigmoid(X_test_manual @ theta)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "accuracy = np.mean(y_pred == y_test_arr) * 100\n",
        "print(f\"\\nManual Logistic Regression Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# Check loss trend\n",
        "initial_loss = losses[0]\n",
        "final_loss = losses[-1]\n",
        "\n",
        "print(f\"Initial Loss: {initial_loss:.6f}\")\n",
        "print(f\"Final Loss: {final_loss:.6f}\")\n",
        "\n",
        "if final_loss < initial_loss:\n",
        "    print(\"Loss decreased over training — Gradient Descent converged properly.\")\n",
        "else:\n",
        "    print(\"Loss did NOT decrease — model did not converge correctly.\")\n",
        "\n",
        "# Optional: show first 5 and last 5 losses to visualize the trend\n",
        "print(\"\\nFirst 5 losses:\", losses[:5])\n",
        "print(\"Last 5 losses:\", losses[-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5lZwDdI8KWX",
        "outputId": "dbff67b3-2fe1-41e9-fa70-ab5aeea7fee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Loss = 0.681028\n",
            "Iteration 100: Loss = 0.281804\n",
            "Iteration 200: Loss = 0.183811\n",
            "Iteration 300: Loss = 0.136463\n",
            "Iteration 400: Loss = 0.108319\n",
            "Iteration 500: Loss = 0.089654\n",
            "Iteration 600: Loss = 0.076383\n",
            "Iteration 700: Loss = 0.066473\n",
            "Iteration 800: Loss = 0.058797\n",
            "Iteration 900: Loss = 0.052682\n",
            "Iteration 999: Loss = 0.047744\n",
            "\n",
            "Minimum Loss: 0.047744 at Iteration 999\n",
            "Final Training Loss: 0.047744\n",
            "\n",
            "First 10 Optimized Weights (theta):\n",
            "[-0.23901885 -0.03960387 -0.03866037 -0.01959266 -0.01343476 -0.00378508\n",
            " -0.01046234 -0.00813789  0.00357438 -0.00628475]\n",
            "\n",
            "Manual Logistic Regression Accuracy: 81.07%\n",
            "Initial Loss: 0.681028\n",
            "Final Loss: 0.047744\n",
            "Loss decreased over training — Gradient Descent converged properly.\n",
            "\n",
            "First 5 losses: [np.float64(0.6810283820870058), np.float64(0.6697831607949025), np.float64(0.6589931830931941), np.float64(0.64860046967171), np.float64(0.6385820077456074)]\n",
            "Last 5 losses: [np.float64(0.047925616514381056), np.float64(0.04788001777606783), np.float64(0.04783450387049092), np.float64(0.04778907456342768), np.float64(0.0477437296215125)]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}