{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "3-way PLSI (User-Document-Word) — EM implementation in NumPy\n",
        "Author: ChatGPT\n",
        "\n",
        "This file implements:\n",
        "- A triadic PLSI model where P(u,d,w) = sum_z P(z) P(u|z) P(d|z) P(w|z)\n",
        "- EM updates with optional Laplace smoothing\n",
        "- A vanilla (document-word) PLSI for comparison\n",
        "- Synthetic data generator, training, held-out perplexity evaluation\n",
        "- Simple user-personalized document retrieval scoring\n",
        "\n",
        "Usage: run the file. All dependencies are standard Python + numpy.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "\n",
        "def normalize_rows(mat):\n",
        "    # normalize rows to sum to 1, avoid divide-by-zero\n",
        "    s = mat.sum(axis=1, keepdims=True)\n",
        "    s[s == 0] = 1.0\n",
        "    return mat / s\n",
        "\n",
        "# ---------------------------\n",
        "# Triadic PLSI model\n",
        "# ---------------------------\n",
        "class TriadicPLSI:\n",
        "    def __init__(self, n_users, n_docs, n_words, n_topics, laplace=1e-6):\n",
        "        self.U = n_users\n",
        "        self.D = n_docs\n",
        "        self.W = n_words\n",
        "        self.Z = n_topics\n",
        "        self.laplace = laplace\n",
        "        # initialize parameters randomly\n",
        "        self.Pz = np.random.rand(self.Z)\n",
        "        self.Pz /= self.Pz.sum()\n",
        "        self.Pu_z = np.random.rand(self.Z, self.U)\n",
        "        self.Pd_z = np.random.rand(self.Z, self.D)\n",
        "        self.Pw_z = np.random.rand(self.Z, self.W)\n",
        "        # normalize conditionals: P(u|z), P(d|z), P(w|z)\n",
        "        self.Pu_z = normalize_rows(self.Pu_z)\n",
        "        self.Pd_z = normalize_rows(self.Pd_z)\n",
        "        self.Pw_z = normalize_rows(self.Pw_z)\n",
        "\n",
        "    def fit(self, triplets, counts=None, n_iters=50, verbose=True, tol=1e-5):\n",
        "        \"\"\"\n",
        "        triplets: list of (u,d,w) tuples (0-based indices)\n",
        "        counts: optional list/array of same length with counts (defaults to 1 each)\n",
        "        \"\"\"\n",
        "        N = len(triplets)\n",
        "        if counts is None:\n",
        "            counts = np.ones(N)\n",
        "        counts = np.asarray(counts, dtype=float)\n",
        "        total_counts = counts.sum()\n",
        "\n",
        "        # Pre-allocate posterior p(z | u,d,w) for each triplet\n",
        "        post = np.zeros((N, self.Z))\n",
        "\n",
        "        prev_ll = -np.inf\n",
        "        for it in range(n_iters):\n",
        "            # E-step: compute posterior p(z | u,d,w)\n",
        "            for i, (u, d, w) in enumerate(triplets):\n",
        "                # unnormalized: P(z) P(u|z) P(d|z) P(w|z)\n",
        "                pz_unnorm = self.Pz * self.Pu_z[:, u] * self.Pd_z[:, d] * self.Pw_z[:, w]\n",
        "                s = pz_unnorm.sum()\n",
        "                if s == 0:\n",
        "                    # assign uniform to avoid NaNs\n",
        "                    post[i, :] = 1.0 / self.Z\n",
        "                else:\n",
        "                    post[i, :] = pz_unnorm / s\n",
        "\n",
        "            # M-step: update Pz, Pu_z, Pd_z, Pw_z using expected counts\n",
        "            # expected count for topic z: sum_i counts[i] * post[i,z]\n",
        "            ez = (counts[:, None] * post).sum(axis=0)  # shape Z\n",
        "            # update priors\n",
        "            self.Pz = ez / ez.sum()\n",
        "\n",
        "            # update conditionals: P(u|z) = sum_{d,w} n(u,d,w) p(z|u,d,w) / ez[z]\n",
        "            Pu_z_new = np.zeros((self.Z, self.U))\n",
        "            Pd_z_new = np.zeros((self.Z, self.D))\n",
        "            Pw_z_new = np.zeros((self.Z, self.W))\n",
        "\n",
        "            for i, (u, d, w) in enumerate(triplets):\n",
        "                c = counts[i]\n",
        "                for z in range(self.Z):\n",
        "                    contrib = c * post[i, z]\n",
        "                    Pu_z_new[z, u] += contrib\n",
        "                    Pd_z_new[z, d] += contrib\n",
        "                    Pw_z_new[z, w] += contrib\n",
        "\n",
        "            # apply Laplace smoothing to avoid zeros\n",
        "            Pu_z_new += self.laplace\n",
        "            Pd_z_new += self.laplace\n",
        "            Pw_z_new += self.laplace\n",
        "\n",
        "            # normalize conditionals\n",
        "            self.Pu_z = normalize_rows(Pu_z_new)\n",
        "            self.Pd_z = normalize_rows(Pd_z_new)\n",
        "            self.Pw_z = normalize_rows(Pw_z_new)\n",
        "\n",
        "            # compute log-likelihood for monitoring\n",
        "            ll = 0.0\n",
        "            for i, (u, d, w) in enumerate(triplets):\n",
        "                prob = (self.Pz * self.Pu_z[:, u] * self.Pd_z[:, d] * self.Pw_z[:, w]).sum()\n",
        "                # numerical stability\n",
        "                if prob <= 0:\n",
        "                    prob = 1e-300\n",
        "                ll += counts[i] * np.log(prob)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Iter {it+1:3d} | Log-likelihood: {ll:.4f}\")\n",
        "\n",
        "            # check convergence on log-likelihood\n",
        "            if abs(ll - prev_ll) < tol:\n",
        "                if verbose:\n",
        "                    print(\"Converged.\")\n",
        "                break\n",
        "            prev_ll = ll\n",
        "\n",
        "    def log_likelihood(self, triplets, counts=None):\n",
        "        if counts is None:\n",
        "            counts = np.ones(len(triplets))\n",
        "        counts = np.asarray(counts, dtype=float)\n",
        "        ll = 0.0\n",
        "        for i, (u, d, w) in enumerate(triplets):\n",
        "            prob = (self.Pz * self.Pu_z[:, u] * self.Pd_z[:, d] * self.Pw_z[:, w]).sum()\n",
        "            if prob <= 0:\n",
        "                prob = 1e-300\n",
        "            ll += counts[i] * np.log(prob)\n",
        "        return ll\n",
        "\n",
        "    def perplexity(self, triplets, counts=None):\n",
        "        if counts is None:\n",
        "            counts = np.ones(len(triplets))\n",
        "        total = counts.sum()\n",
        "        ll = self.log_likelihood(triplets, counts)\n",
        "        return math.exp(-ll / total)\n",
        "\n",
        "    def user_topic_distribution(self, u):\n",
        "        # P(z|u) proportional to P(z) P(u|z)\n",
        "        unnorm = self.Pz * self.Pu_z[:, u]\n",
        "        s = unnorm.sum()\n",
        "        if s == 0:\n",
        "            return np.ones(self.Z) / self.Z\n",
        "        return unnorm / s\n",
        "\n",
        "    def score_docs_for_user(self, u):\n",
        "        # score P(d | u) ∝ sum_z P(z|u) P(d|z)\n",
        "        pz_given_u = self.user_topic_distribution(u)\n",
        "        # P(d|u) = sum_z P(d|z) P(z|u)\n",
        "        scores = (pz_given_u[:, None] * self.Pd_z).sum(axis=0)\n",
        "        return scores\n",
        "\n",
        "# ---------------------------\n",
        "# Vanilla PLSI (Document-Word)\n",
        "# ---------------------------\n",
        "class VanillaPLSI:\n",
        "    def __init__(self, n_docs, n_words, n_topics, laplace=1e-6):\n",
        "        self.D = n_docs\n",
        "        self.W = n_words\n",
        "        self.Z = n_topics\n",
        "        self.laplace = laplace\n",
        "        self.Pz = np.random.rand(self.Z)\n",
        "        self.Pz /= self.Pz.sum()\n",
        "        self.Pd_z = np.random.rand(self.Z, self.D)\n",
        "        self.Pw_z = np.random.rand(self.Z, self.W)\n",
        "        self.Pd_z = normalize_rows(self.Pd_z)\n",
        "        self.Pw_z = normalize_rows(self.Pw_z)\n",
        "\n",
        "    def fit(self, pairs, counts=None, n_iters=50, verbose=True, tol=1e-5):\n",
        "        if counts is None:\n",
        "            counts = np.ones(len(pairs))\n",
        "        counts = np.asarray(counts, dtype=float)\n",
        "        N = len(pairs)\n",
        "        post = np.zeros((N, self.Z))\n",
        "        prev_ll = -np.inf\n",
        "        for it in range(n_iters):\n",
        "            # E-step\n",
        "            for i, (d, w) in enumerate(pairs):\n",
        "                pz_unnorm = self.Pz * self.Pd_z[:, d] * self.Pw_z[:, w]\n",
        "                s = pz_unnorm.sum()\n",
        "                if s == 0:\n",
        "                    post[i, :] = 1.0 / self.Z\n",
        "                else:\n",
        "                    post[i, :] = pz_unnorm / s\n",
        "\n",
        "            ez = (counts[:, None] * post).sum(axis=0)\n",
        "            self.Pz = ez / ez.sum()\n",
        "\n",
        "            Pd_z_new = np.zeros((self.Z, self.D))\n",
        "            Pw_z_new = np.zeros((self.Z, self.W))\n",
        "            for i, (d, w) in enumerate(pairs):\n",
        "                c = counts[i]\n",
        "                for z in range(self.Z):\n",
        "                    contrib = c * post[i, z]\n",
        "                    Pd_z_new[z, d] += contrib\n",
        "                    Pw_z_new[z, w] += contrib\n",
        "            Pd_z_new += self.laplace\n",
        "            Pw_z_new += self.laplace\n",
        "            self.Pd_z = normalize_rows(Pd_z_new)\n",
        "            self.Pw_z = normalize_rows(Pw_z_new)\n",
        "\n",
        "            ll = 0.0\n",
        "            for i, (d, w) in enumerate(pairs):\n",
        "                prob = (self.Pz * self.Pd_z[:, d] * self.Pw_z[:, w]).sum()\n",
        "                if prob <= 0:\n",
        "                    prob = 1e-300\n",
        "                ll += counts[i] * np.log(prob)\n",
        "            if verbose:\n",
        "                print(f\"Vanilla Iter {it+1:3d} | LL: {ll:.4f}\")\n",
        "            if abs(ll - prev_ll) < tol:\n",
        "                if verbose:\n",
        "                    print(\"Vanilla converged\")\n",
        "                break\n",
        "            prev_ll = ll\n",
        "\n",
        "    def log_likelihood(self, pairs, counts=None):\n",
        "        if counts is None:\n",
        "            counts = np.ones(len(pairs))\n",
        "        counts = np.asarray(counts, dtype=float)\n",
        "        ll = 0.0\n",
        "        for i, (d, w) in enumerate(pairs):\n",
        "            prob = (self.Pz * self.Pd_z[:, d] * self.Pw_z[:, w]).sum()\n",
        "            if prob <= 0:\n",
        "                prob = 1e-300\n",
        "            ll += counts[i] * np.log(prob)\n",
        "        return ll\n",
        "\n",
        "    def perplexity(self, pairs, counts=None):\n",
        "        if counts is None:\n",
        "            counts = np.ones(len(pairs))\n",
        "        total = counts.sum()\n",
        "        ll = self.log_likelihood(pairs, counts)\n",
        "        return math.exp(-ll / total)\n",
        "\n",
        "# ---------------------------\n",
        "# Synthetic data generator\n",
        "# ---------------------------\n",
        "\n",
        "def generate_synthetic(U=20, D=50, W=100, Z=5, N=5000, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Generate synthetic triplets from a known triadic model\n",
        "    Returns: triplets list, counts (all ones), and ground-truth parameters\n",
        "    \"\"\"\n",
        "    # ground truth topic prior\n",
        "    Pz_true = np.random.dirichlet([alpha]*Z)\n",
        "    Pu_z_true = np.array([np.random.dirichlet([alpha]*U) for _ in range(Z)])\n",
        "    Pd_z_true = np.array([np.random.dirichlet([alpha]*D) for _ in range(Z)])\n",
        "    Pw_z_true = np.array([np.random.dirichlet([alpha]*W) for _ in range(Z)])\n",
        "\n",
        "    triplets = []\n",
        "    counts = []\n",
        "    for n in range(N):\n",
        "        z = np.random.choice(Z, p=Pz_true)\n",
        "        u = np.random.choice(U, p=Pu_z_true[z])\n",
        "        d = np.random.choice(D, p=Pd_z_true[z])\n",
        "        w = np.random.choice(W, p=Pw_z_true[z])\n",
        "        triplets.append((u, d, w))\n",
        "        counts.append(1)\n",
        "\n",
        "    return triplets, np.array(counts), (Pz_true, Pu_z_true, Pd_z_true, Pw_z_true)\n",
        "\n",
        "# ---------------------------\n",
        "# Demo training & evaluation\n",
        "# ---------------------------\n",
        "if __name__ == '__main__':\n",
        "    # create synthetic data (train/test split)\n",
        "    triplets, counts, gt = generate_synthetic(U=15, D=40, W=200, Z=6, N=4000)\n",
        "    # simple train-test split\n",
        "    split = int(0.8 * len(triplets))\n",
        "    train_trip = triplets[:split]\n",
        "    train_counts = counts[:split]\n",
        "    test_trip = triplets[split:]\n",
        "    test_counts = counts[split:]\n",
        "\n",
        "    # train triadic model\n",
        "    model = TriadicPLSI(n_users=15, n_docs=40, n_words=200, n_topics=6, laplace=1e-6)\n",
        "    print('\\nTraining Triadic PLSI...')\n",
        "    model.fit(train_trip, counts=train_counts, n_iters=30, verbose=True)\n",
        "    print('\\nTriadic Perplexity on test set:', model.perplexity(test_trip, counts=test_counts))\n",
        "\n",
        "    # prepare pairs (d,w) for vanilla model by dropping user dimension\n",
        "    pairs = [(d, w) for (u, d, w) in train_trip]\n",
        "    pairs_test = [(d, w) for (u, d, w) in test_trip]\n",
        "\n",
        "    vanilla = VanillaPLSI(n_docs=40, n_words=200, n_topics=6, laplace=1e-6)\n",
        "    print('\\nTraining Vanilla (doc-word) PLSI...')\n",
        "    vanilla.fit(pairs, counts=train_counts, n_iters=30, verbose=True)\n",
        "    print('\\nVanilla Perplexity on test set:', vanilla.perplexity(pairs_test, counts=test_counts))\n",
        "\n",
        "    # simple retrieval: given user u, rank documents\n",
        "    u = 3\n",
        "    scores = model.score_docs_for_user(u)\n",
        "    top_docs = np.argsort(-scores)[:10]\n",
        "    print(f\"\\nTop docs for user {u} (by model):\", top_docs)\n",
        "\n",
        "    # show top words for each topic\n",
        "    topk = 8\n",
        "    for z in range(model.Z):\n",
        "        top_words = np.argsort(-model.Pw_z[z])[:topk]\n",
        "        print(f\"Topic {z}: top words indices: {top_words}\")\n",
        "\n",
        "    print('\\nDone.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyfEx2SZckl5",
        "outputId": "dc537d6b-9aa8-4205-b7f7-11516a2ad8c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Triadic PLSI...\n",
            "Iter   1 | Log-likelihood: -35212.8762\n",
            "Iter   2 | Log-likelihood: -35048.6161\n",
            "Iter   3 | Log-likelihood: -34883.5497\n",
            "Iter   4 | Log-likelihood: -34717.0272\n",
            "Iter   5 | Log-likelihood: -34563.1429\n",
            "Iter   6 | Log-likelihood: -34423.8763\n",
            "Iter   7 | Log-likelihood: -34301.1691\n",
            "Iter   8 | Log-likelihood: -34195.2452\n",
            "Iter   9 | Log-likelihood: -34104.6745\n",
            "Iter  10 | Log-likelihood: -34027.2016\n",
            "Iter  11 | Log-likelihood: -33964.8974\n",
            "Iter  12 | Log-likelihood: -33916.7690\n",
            "Iter  13 | Log-likelihood: -33878.5709\n",
            "Iter  14 | Log-likelihood: -33847.1656\n",
            "Iter  15 | Log-likelihood: -33821.2107\n",
            "Iter  16 | Log-likelihood: -33799.5880\n",
            "Iter  17 | Log-likelihood: -33780.7731\n",
            "Iter  18 | Log-likelihood: -33763.4074\n",
            "Iter  19 | Log-likelihood: -33746.2870\n",
            "Iter  20 | Log-likelihood: -33727.6949\n",
            "Iter  21 | Log-likelihood: -33706.0327\n",
            "Iter  22 | Log-likelihood: -33682.9630\n",
            "Iter  23 | Log-likelihood: -33661.5395\n",
            "Iter  24 | Log-likelihood: -33641.1361\n",
            "Iter  25 | Log-likelihood: -33620.9273\n",
            "Iter  26 | Log-likelihood: -33603.7545\n",
            "Iter  27 | Log-likelihood: -33590.3196\n",
            "Iter  28 | Log-likelihood: -33579.8194\n",
            "Iter  29 | Log-likelihood: -33571.0734\n",
            "Iter  30 | Log-likelihood: -33563.1031\n",
            "\n",
            "Triadic Perplexity on test set: 65491.907195239\n",
            "\n",
            "Training Vanilla (doc-word) PLSI...\n",
            "Vanilla Iter   1 | LL: -27120.6311\n",
            "Vanilla Iter   2 | LL: -27068.5514\n",
            "Vanilla Iter   3 | LL: -27015.2473\n",
            "Vanilla Iter   4 | LL: -26954.0940\n",
            "Vanilla Iter   5 | LL: -26885.0233\n",
            "Vanilla Iter   6 | LL: -26813.5588\n",
            "Vanilla Iter   7 | LL: -26746.1744\n",
            "Vanilla Iter   8 | LL: -26687.1942\n",
            "Vanilla Iter   9 | LL: -26637.1907\n",
            "Vanilla Iter  10 | LL: -26594.9047\n",
            "Vanilla Iter  11 | LL: -26558.9209\n",
            "Vanilla Iter  12 | LL: -26527.9147\n",
            "Vanilla Iter  13 | LL: -26500.6448\n",
            "Vanilla Iter  14 | LL: -26476.2941\n",
            "Vanilla Iter  15 | LL: -26454.3913\n",
            "Vanilla Iter  16 | LL: -26434.8411\n",
            "Vanilla Iter  17 | LL: -26417.5444\n",
            "Vanilla Iter  18 | LL: -26402.1011\n",
            "Vanilla Iter  19 | LL: -26388.2327\n",
            "Vanilla Iter  20 | LL: -26376.0220\n",
            "Vanilla Iter  21 | LL: -26365.1751\n",
            "Vanilla Iter  22 | LL: -26355.3060\n",
            "Vanilla Iter  23 | LL: -26346.2837\n",
            "Vanilla Iter  24 | LL: -26338.1163\n",
            "Vanilla Iter  25 | LL: -26330.7729\n",
            "Vanilla Iter  26 | LL: -26324.1208\n",
            "Vanilla Iter  27 | LL: -26317.9642\n",
            "Vanilla Iter  28 | LL: -26312.1480\n",
            "Vanilla Iter  29 | LL: -26306.6321\n",
            "Vanilla Iter  30 | LL: -26301.4523\n",
            "\n",
            "Vanilla Perplexity on test set: 6588.021497924444\n",
            "\n",
            "Top docs for user 3 (by model): [24 36 27  7 37 12 34 17 13 28]\n",
            "Topic 0: top words indices: [ 99  28  80  89 105 116  77 108]\n",
            "Topic 1: top words indices: [105  29 197 158  45  96 179 188]\n",
            "Topic 2: top words indices: [102  11  28  31 187 167 146 143]\n",
            "Topic 3: top words indices: [ 77  17 102   0  95 170 101 148]\n",
            "Topic 4: top words indices: [140 152 112  68 137  74  40  28]\n",
            "Topic 5: top words indices: [ 14   4   7 142  42  98  96  39]\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    }
  ]
}