{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Cross-Domain Sentiment Classification (Books â†’ Electronics)\n",
        "# Using Spectral Feature Alignment (SFA)\n",
        "# =====================================================\n",
        "\n",
        "!pip install numpy scipy scikit-learn matplotlib nltk tqdm --quiet\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "import nltk, re, urllib.request, zipfile, os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# -----------------------------\n",
        "# 1ï¸âƒ£ Download & Prepare Dataset\n",
        "# -----------------------------\n",
        "# Amazon review dataset from Blitzer et al. (domain adaptation)\n",
        "url = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/processed_acl.tar.gz\"\n",
        "if not os.path.exists(\"processed_acl.tar.gz\"):\n",
        "    urllib.request.urlretrieve(url, \"processed_acl.tar.gz\")\n",
        "\n",
        "import tarfile\n",
        "# Ensure extraction to the current directory or a specified path\n",
        "extract_path = \".\"\n",
        "if not os.path.exists(\"processed_acl\"):\n",
        "    with tarfile.open(\"processed_acl.tar.gz\", \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "# Helper to load data from a domain folder\n",
        "def load_domain(domain, label_limit=1000):\n",
        "    # Corrected paths based on the observed extracted structure\n",
        "    pos_files = [os.path.join(extract_path, f\"processed_acl/{domain}/positive.review\")]\n",
        "    neg_files = [os.path.join(extract_path, f\"processed_acl/{domain}/negative.review\")]\n",
        "\n",
        "    pos_texts = []\n",
        "    for f_path in pos_files:\n",
        "        if os.path.exists(f_path):\n",
        "            with open(f_path, encoding=\"latin1\") as f:\n",
        "                pos_texts.extend(f.readlines()[:label_limit])\n",
        "\n",
        "    neg_texts = []\n",
        "    for f_path in neg_files:\n",
        "        if os.path.exists(f_path):\n",
        "            with open(f_path, encoding=\"latin1\") as f:\n",
        "                neg_texts.extend(f.readlines()[:label_limit])\n",
        "\n",
        "    texts = [text.strip() for text in pos_texts + neg_texts]\n",
        "    labels = np.array([1]*len(pos_texts) + [0]*len(neg_texts))\n",
        "    return texts, labels\n",
        "\n",
        "source_texts, source_labels = load_domain(\"books\")\n",
        "target_texts, target_labels = load_domain(\"electronics\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2ï¸âƒ£ Text Cleaning & Vectorizing\n",
        "# -----------------------------\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z ]', ' ', text.lower())\n",
        "    return ' '.join([w for w in text.split() if w not in stop_words and len(w) > 2])\n",
        "\n",
        "source_texts = [clean_text(t) for t in tqdm(source_texts)]\n",
        "target_texts = [clean_text(t) for t in tqdm(target_texts)]\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000, binary=True)\n",
        "X_source = vectorizer.fit_transform(source_texts)\n",
        "X_target = vectorizer.transform(target_texts)\n",
        "vocab = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# -----------------------------\n",
        "# 3ï¸âƒ£ Identify Domain-Independent vs Domain-Specific Words\n",
        "# -----------------------------\n",
        "freq_source = np.array((X_source > 0).sum(axis=0)).ravel()\n",
        "freq_target = np.array((X_target > 0).sum(axis=0)).ravel()\n",
        "\n",
        "mask_independent = (freq_source > 30) & (freq_target > 30)\n",
        "domain_independent = vocab[mask_independent]\n",
        "domain_specific = vocab[~mask_independent]\n",
        "\n",
        "print(f\"Domain-independent words: {len(domain_independent)}\")\n",
        "print(f\"Domain-specific words: {len(domain_specific)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4ï¸âƒ£ Construct Co-occurrence Matrix (Bipartite Graph)\n",
        "# -----------------------------\n",
        "# For simplicity, co-occurrence = how often both appear in same document\n",
        "M = np.zeros((len(domain_specific), len(domain_independent)))\n",
        "# Create mappings for faster lookups\n",
        "ds_vocab_map = {word: i for i, word in enumerate(domain_specific)}\n",
        "di_vocab_map = {word: j for j, word in enumerate(domain_independent)}\n",
        "\n",
        "# Combine source and target texts and their corresponding feature matrices for co-occurrence calculation\n",
        "all_texts_combined = source_texts + target_texts\n",
        "X_combined = vectorizer.transform(all_texts_combined)\n",
        "\n",
        "for doc_idx in range(X_combined.shape[0]):\n",
        "    doc_features = X_combined[doc_idx, :].nonzero()[1] # Get indices of words present in the document\n",
        "\n",
        "    ds_present_indices = [ds_vocab_map[vocab[idx]] for idx in doc_features if vocab[idx] in ds_vocab_map]\n",
        "    di_present_indices = [di_vocab_map[vocab[idx]] for idx in doc_features if vocab[idx] in di_vocab_map]\n",
        "\n",
        "    for ds_map_idx in ds_present_indices:\n",
        "        for di_map_idx in di_present_indices:\n",
        "            M[ds_map_idx, di_map_idx] += 1\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5ï¸âƒ£ Spectral Clustering for Alignment\n",
        "# -----------------------------\n",
        "n_clusters = 100\n",
        "# Adjusted affinity to handle potential zero rows in M\n",
        "sc = SpectralClustering(n_clusters=n_clusters, affinity='cosine', assign_labels='kmeans', random_state=42)\n",
        "# Handle cases where M might have zero rows (no domain-specific words co-occurring with DI words)\n",
        "if M.shape[0] > 0 and np.sum(M) > 0:\n",
        "    feature_clusters = sc.fit_predict(M)\n",
        "else:\n",
        "    # Assign all DS features to a single cluster if M is empty or all zeros\n",
        "    feature_clusters = np.zeros(len(domain_specific), dtype=int)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6ï¸âƒ£ Create Augmented Feature Representation\n",
        "# -----------------------------\n",
        "def augment_features(X, vocab, domain_specific, feature_clusters, n_clusters, vectorizer):\n",
        "    X_new = np.zeros((X.shape[0], n_clusters))\n",
        "    ds_word_to_cluster = {word: cluster for word, cluster in zip(domain_specific, feature_clusters)}\n",
        "\n",
        "    for i, word in enumerate(vocab):\n",
        "        if word in ds_word_to_cluster:\n",
        "            cluster = ds_word_to_cluster[word]\n",
        "            if word in vectorizer.vocabulary_:\n",
        "                 idx = vectorizer.vocabulary_[word]\n",
        "                 X_new[:, cluster] += X[:, idx].toarray().ravel()\n",
        "    return np.hstack([X.toarray(), 0.6 * X_new])\n",
        "\n",
        "X_source_aug = augment_features(X_source, vocab, domain_specific, feature_clusters, n_clusters, vectorizer)\n",
        "X_target_aug = augment_features(X_target, vocab, domain_specific, feature_clusters, n_clusters, vectorizer)\n",
        "\n",
        "# -----------------------------\n",
        "# 7ï¸âƒ£ Train on Source â†’ Test on Target\n",
        "# -----------------------------\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_source_aug, source_labels)\n",
        "preds = clf.predict(X_target_aug)\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report (Books â†’ Electronics):\")\n",
        "print(classification_report(target_labels, preds))\n",
        "\n",
        "# -----------------------------\n",
        "# 8ï¸âƒ£ Inspect Learned Clusters\n",
        "# -----------------------------\n",
        "print(\"\\nðŸ” Example Word Clusters:\")\n",
        "# Ensure there are clusters before attempting to print\n",
        "if n_clusters > 0 and len(domain_specific) > 0:\n",
        "    for c in range(min(5, n_clusters)): # Print up to 5 clusters\n",
        "        words_in_cluster = domain_specific[feature_clusters == c][:10]\n",
        "        if len(words_in_cluster) > 0:\n",
        "            print(f\"Cluster {c}: {', '.join(words_in_cluster)}\")\n",
        "        else:\n",
        "            print(f\"Cluster {c}: No domain-specific words in this cluster.\")\n",
        "else:\n",
        "    print(\"No clusters formed or no domain-specific words available to cluster.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEEXJH_Q-nOp",
        "outputId": "175e1634-7ce3-4bcf-fb80-e73443c3f14a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/tmp/ipython-input-1439152371.py:35: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=extract_path)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1855.62it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2070.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain-independent words: 363\n",
            "Domain-specific words: 4637\n",
            "\n",
            "ðŸ“Š Classification Report (Books â†’ Electronics):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00      1000\n",
            "           1       1.00      0.99      1.00      1000\n",
            "\n",
            "    accuracy                           1.00      2000\n",
            "   macro avg       1.00      1.00      1.00      2000\n",
            "weighted avg       1.00      1.00      1.00      2000\n",
            "\n",
            "\n",
            "ðŸ” Example Word Clusters:\n",
            "Cluster 0: admire, argued, australia, builds, cats, compromise, creatures, ethics, experimentation, experiments\n",
            "Cluster 1: action, adams, additionally, admirable, admiration, adventure, affair, alert, alex, alternate\n",
            "Cluster 2: absurd, accurately, acquire, applied, arguments, belief, believes, challenged, christians, claimed\n",
            "Cluster 3: angst, bore, classes, clues, continually, decline, episode, frozen, lying, morality\n",
            "Cluster 4: acquainted, afterwards, agreement, aids, applicable, assertion, benefits, compiled, condition, considerations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gtz0hfY0_mON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}