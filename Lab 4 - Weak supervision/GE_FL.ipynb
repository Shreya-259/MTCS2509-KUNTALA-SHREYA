{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GE-FL: Generalized Expectation with Feature Labels\n",
        "Reference: \"Learning from Labeled Features using Generalized Expectation Criteria\"\n",
        "           Gregory Druck, Gideon Mann, Andrew McCallum. SIGIR 2008.\n",
        "\n",
        "This script:\n",
        " - loads a small subset of 20newsgroups (ibm vs mac)\n",
        " - extracts a bag-of-words feature matrix\n",
        " - constructs candidate features and a simulated 'oracle' feature labeler\n",
        " - trains a multinomial logistic regression model by minimizing:\n",
        "       Loss = sum_k KL(p_hat_k || p_tilde_k(theta))  +  (1/(2*sigma^2)) * ||theta||^2\n",
        "   where p_tilde_k(theta) is the model predicted class distribution over all\n",
        "   unlabeled instances that contain feature k, and p_hat_k is a reference distribution\n",
        "   for feature k (from labeled features).\n",
        " - evaluates the trained model on a held-out labeled test set (for demonstration).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Utilities\n",
        "# ------------------------\n",
        "def softmax(logits):\n",
        "    z = logits - logits.max(axis=1, keepdims=True)\n",
        "    expz = np.exp(z)\n",
        "    return expz / expz.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def build_schapire_distributions(feature_labels, n_classes, qmaj=0.9):\n",
        "    \"\"\"\n",
        "    Create reference distributions for each labeled feature using Schapire heuristic.\n",
        "    \"\"\"\n",
        "    p_hat_dict = {}\n",
        "    for k, assoc in feature_labels.items():\n",
        "        assoc = list(assoc)\n",
        "        n_assoc = len(assoc)\n",
        "        p = np.ones(n_classes) * ((1.0 - qmaj) / max(1, (n_classes - n_assoc)))\n",
        "        if n_assoc > 0:\n",
        "            p[assoc] = qmaj / n_assoc\n",
        "        p_hat_dict[k] = p\n",
        "    return p_hat_dict\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# GE-FL objective + gradient\n",
        "# ------------------------\n",
        "class GEFLTrainer:\n",
        "    def __init__(self, X_unlabeled, n_classes, feature_p_hats, sigma=1.0, add_bias=True):\n",
        "        self.X = X_unlabeled.tocsr(copy=False)\n",
        "        self.n_samples, self.n_features = self.X.shape\n",
        "        self.n_classes = n_classes\n",
        "        self.feature_p_hats = feature_p_hats\n",
        "        self.sigma = float(sigma)\n",
        "        self.add_bias = add_bias\n",
        "\n",
        "        # Precompute rows where each labeled feature occurs\n",
        "        self.feat_rows = {k: self.X[:, k].nonzero()[0] for k in feature_p_hats.keys()}\n",
        "\n",
        "    def pack_params(self, W, b):\n",
        "        return np.concatenate([W.ravel(), b.ravel()]) if self.add_bias else W.ravel()\n",
        "\n",
        "    def unpack_params(self, params):\n",
        "        if self.add_bias:\n",
        "            size_w = self.n_classes * self.n_features\n",
        "            W = params[:size_w].reshape(self.n_classes, self.n_features)\n",
        "            b = params[size_w:size_w + self.n_classes]\n",
        "            return W, b\n",
        "        else:\n",
        "            W = params.reshape(self.n_classes, self.n_features)\n",
        "            b = np.zeros(self.n_classes)\n",
        "            return W, b\n",
        "\n",
        "    def predict_proba_unlabeled(self, W, b):\n",
        "        logits = self.X.dot(W.T)\n",
        "        if self.add_bias:\n",
        "            logits = logits + b.reshape(1, -1)\n",
        "        return softmax(logits)\n",
        "\n",
        "    def objective_and_grad(self, params):\n",
        "        W, b = self.unpack_params(params)\n",
        "        P = self.predict_proba_unlabeled(W, b)\n",
        "\n",
        "        loss = 0.0\n",
        "        grad_W = np.zeros_like(W)\n",
        "        grad_b = np.zeros_like(b)\n",
        "\n",
        "        for k, p_hat in self.feature_p_hats.items():\n",
        "            rows = self.feat_rows.get(k, np.array([], dtype=int))\n",
        "            Ck = len(rows)\n",
        "            if Ck == 0:\n",
        "                continue\n",
        "\n",
        "            Q = P[rows]                      # (Ck, n_classes)\n",
        "            p_tilde = Q.mean(axis=0)         # model expectation for this feature\n",
        "            eps = 1e-12\n",
        "            p_tilde = np.clip(p_tilde, eps, 1.0)\n",
        "            p_tilde = p_tilde / p_tilde.sum()\n",
        "\n",
        "            p_hat = np.asarray(p_hat, dtype=float)\n",
        "            p_hat = np.clip(p_hat, eps, 1.0)\n",
        "            p_hat = p_hat / p_hat.sum()\n",
        "\n",
        "            # KL divergence\n",
        "            kl = np.sum(p_hat * (np.log(p_hat) - np.log(p_tilde)))\n",
        "            loss += kl\n",
        "\n",
        "            # Gradient\n",
        "            alpha = p_hat / p_tilde\n",
        "            s = Q.dot(alpha)                       # (Ck,)\n",
        "            A = Q * alpha[np.newaxis, :]           # (Ck, n_classes)\n",
        "            A_final = - (1.0 / float(Ck)) * (A - (Q * s[:, np.newaxis]))\n",
        "\n",
        "            X_rows = self.X[rows].toarray()        # force dense (Ck, n_features)\n",
        "            grad_W += (A_final.T @ X_rows).astype(np.float64)\n",
        "            grad_b += A_final.sum(axis=0)\n",
        "        # Gaussian prior\n",
        "        prior_term = 0.5 * (np.sum(W * W) + np.sum(b * b)) / (self.sigma ** 2)\n",
        "        loss += prior_term\n",
        "        grad_W += W / (self.sigma ** 2)\n",
        "        grad_b += b / (self.sigma ** 2)\n",
        "\n",
        "        grad = np.concatenate([grad_W.ravel(), grad_b.ravel()]) if self.add_bias else grad_W.ravel()\n",
        "        return loss, grad\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# Example pipeline\n",
        "# ------------------------\n",
        "def example_run():\n",
        "    categories = ['comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
        "    data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "    y_all = data.target\n",
        "    X_text = data.data\n",
        "\n",
        "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "        X_text, y_all, test_size=0.3, random_state=42, stratify=y_all\n",
        "    )\n",
        "\n",
        "    vect = CountVectorizer(min_df=5, stop_words='english', max_features=5000)\n",
        "    X_train_counts = vect.fit_transform(X_train_text)\n",
        "    X_test_counts = vect.transform(X_test_text)\n",
        "    n_classes = len(np.unique(y_all))\n",
        "\n",
        "    # Oracle feature selection (mutual information)\n",
        "    mi = mutual_info_classif(X_train_counts, y_train, discrete_features=True)\n",
        "    top_k = 100\n",
        "    top_idx = np.argsort(mi)[::-1][:top_k]\n",
        "\n",
        "    feature_labels = {}\n",
        "    Xc = X_train_counts.tocsr()\n",
        "    for k in top_idx:\n",
        "        rows = Xc[:, k].nonzero()[0]\n",
        "        if len(rows) == 0:\n",
        "            continue\n",
        "        labels_in_rows = y_train[rows]\n",
        "        counts = np.bincount(labels_in_rows, minlength=n_classes)\n",
        "        max_class = np.argmax(counts)\n",
        "        assoc = [max_class]\n",
        "        for c in range(n_classes):\n",
        "            if c != max_class and counts[c] >= 0.5 * counts[max_class]:\n",
        "                assoc.append(c)\n",
        "        if counts[max_class] > 0:\n",
        "            feature_labels[k] = assoc\n",
        "\n",
        "    print(f\"Selected {len(feature_labels)} labeled features (oracle-simulated).\")\n",
        "\n",
        "    # Build reference distributions\n",
        "    p_hat_dict = build_schapire_distributions(feature_labels, n_classes, qmaj=0.9)\n",
        "\n",
        "    # Train GE-FL\n",
        "    trainer = GEFLTrainer(X_train_counts, n_classes, p_hat_dict, sigma=1.0, add_bias=True)\n",
        "\n",
        "    rng = np.random.RandomState(0)\n",
        "    W0 = 0.01 * rng.randn(n_classes, X_train_counts.shape[1])\n",
        "    b0 = np.zeros(n_classes)\n",
        "    p0 = trainer.pack_params(W0, b0)\n",
        "\n",
        "    def fun_and_grad(p):\n",
        "        loss, grad = trainer.objective_and_grad(p)\n",
        "        return loss, grad\n",
        "\n",
        "    print(\"Starting L-BFGS optimization...\")\n",
        "    res = minimize(fun_and_grad, p0, method='L-BFGS-B', jac=True,\n",
        "                   options={'maxiter': 50, 'disp': True})  # reduce maxiter for demo\n",
        "    W_opt, b_opt = trainer.unpack_params(res.x)\n",
        "    print(\"Optimization done. Final objective:\", res.fun)\n",
        "\n",
        "    # Evaluate\n",
        "    logits_test = X_test_counts.dot(W_opt.T) + b_opt.reshape(1, -1)\n",
        "    P_test = softmax(logits_test)\n",
        "    y_pred = np.argmax(P_test, axis=1)\n",
        "    print(\"Test accuracy (GE-FL):\", accuracy_score(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred, target_names=categories))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_run()"
      ],
      "metadata": {
        "id": "3dFY33n3z9s8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc47a060-e769-4e96-bd87-83fc0e815e0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 100 labeled features (oracle-simulated).\n",
            "Starting L-BFGS optimization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3791139081.py:192: DeprecationWarning: scipy.optimize: The `disp` and `iprint` options of the L-BFGS-B solver are deprecated and will be removed in SciPy 1.18.0.\n",
            "  res = minimize(fun_and_grad, p0, method='L-BFGS-B', jac=True,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization done. Final objective: 5.302976412435929\n",
            "Test accuracy (GE-FL): 0.791095890410959\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "comp.sys.ibm.pc.hardware       0.73      0.94      0.82       295\n",
            "   comp.sys.mac.hardware       0.91      0.64      0.75       289\n",
            "\n",
            "                accuracy                           0.79       584\n",
            "               macro avg       0.82      0.79      0.79       584\n",
            "            weighted avg       0.82      0.79      0.79       584\n",
            "\n"
          ]
        }
      ]
    }
  ]
}